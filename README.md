# Explainable AI for Transparent Decision Making in Healthcare
## Heart Disease Prediction with SHAP & LIME

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Research%20Prototype-orange)

---

## ðŸ“Œ Project Overview

This project implements a **modular Explainable AI (XAI) framework** for heart disease prediction using the UCI Heart Disease dataset. It goes beyond standard ML pipelines by integrating **SHAP** and **LIME** explainability techniques, enabling clinicians and researchers to understand *why* a model makes a particular prediction â€” not just *what* it predicts.

### Research Motivation

Healthcare AI systems must be **transparent and trustworthy**. Black-box models, despite high accuracy, are unsuitable for clinical adoption because:

- Clinicians need to **verify** AI reasoning against medical knowledge
- Patients have a right to **understand** decisions affecting their health
- Regulatory frameworks (e.g., EU AI Act) require **explainability** for high-risk AI

This project demonstrates how XAI techniques can bridge the gap between predictive performance and clinical interpretability.

---

## ðŸ—ï¸ Architecture

```
heart_xai_project/
â”‚
â”œâ”€â”€ data/                       # Dataset storage
â”‚   â””â”€â”€ heart.csv
â”‚
â”œâ”€â”€ models/                     # Saved trained models (.joblib)
â”‚
â”œâ”€â”€ preprocessing/              # Modular preprocessing pipeline
â”‚   â”œâ”€â”€ __init__.py             # Pipeline orchestrator
â”‚   â”œâ”€â”€ imputation.py           # Missing value imputation
â”‚   â”œâ”€â”€ outlier.py              # IQR outlier removal
â”‚   â”œâ”€â”€ scaling.py              # Z-score standardization
â”‚   â”œâ”€â”€ smote.py                # SMOTE oversampling
â”‚   â””â”€â”€ pca.py                  # PCA dimensionality reduction
â”‚
â”œâ”€â”€ training/                   # Model training & evaluation
â”‚   â”œâ”€â”€ train_models.py         # Train LR, RF, XGBoost
â”‚   â””â”€â”€ evaluate.py             # Metrics computation & comparison
â”‚
â”œâ”€â”€ explainability/             # XAI modules
â”‚   â”œâ”€â”€ shap_explainer.py       # SHAP global & local explanations
â”‚   â””â”€â”€ lime_explainer.py       # LIME local explanations
â”‚
â”œâ”€â”€ app/                        # Streamlit dashboard
â”‚   â””â”€â”€ streamlit_app.py        # Multi-page web interface
â”‚
â”œâ”€â”€ utils/                      # Utility functions
â”‚   â””â”€â”€ helpers.py              # Data loading, path constants
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ”§ Installation

### Prerequisites
- Python 3.9 or higher
- pip package manager

### Steps

```bash
# 1. Clone or navigate to the project
cd heart_xai_project

# 2. Create a virtual environment (recommended)
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # macOS/Linux

# 3. Install dependencies
pip install -r requirements.txt
```

---

## ðŸš€ How to Run

### Step 1: Train the Models

```bash
python training/train_models.py
```

This will:
- Load and preprocess the UCI Heart Disease dataset
- Train Logistic Regression, Random Forest, and XGBoost models
- Evaluate all models and select the best performer
- Save trained models to `models/`

### Step 2: Launch the Dashboard

```bash
streamlit run app/streamlit_app.py
```

Navigate to `http://localhost:8501` to explore:
- ðŸ“Š **Dataset Overview** â€” Feature distributions & class balance
- ðŸ¤– **Model Performance** â€” Metrics comparison & ROC curves
- â¤ï¸ **Prediction Interface** â€” Input patient data & get predictions
- ðŸ” **Explainability Dashboard** â€” SHAP & LIME visual explanations
- ðŸ“ˆ **Ethical Insights** â€” Bias awareness & clinical considerations

---

## ðŸ“Š Dataset

**UCI Heart Disease Dataset** (Cleveland subset)
- **Samples**: 303
- **Features**: 13 clinical attributes
- **Target**: Binary (0 = No Disease, 1 = Disease)

| Feature | Description |
|---------|-------------|
| age | Age in years |
| sex | Sex (1 = male, 0 = female) |
| cp | Chest pain type (0â€“3) |
| trestbps | Resting blood pressure (mm Hg) |
| chol | Serum cholesterol (mg/dl) |
| fbs | Fasting blood sugar > 120 mg/dl |
| restecg | Resting ECG results (0â€“2) |
| thalach | Maximum heart rate achieved |
| exang | Exercise-induced angina |
| oldpeak | ST depression induced by exercise |
| slope | Slope of peak exercise ST segment |
| ca | Number of major vessels colored by fluoroscopy |
| thal | Thalassemia (0 = normal, 1 = fixed defect, 2 = reversible defect) |

---

## ðŸ§  Explainability Techniques

### SHAP (SHapley Additive exPlanations)
- **Global**: Feature importance ranking across the entire dataset
- **Local**: Per-instance force plots showing how each feature pushes the prediction

### LIME (Local Interpretable Model-agnostic Explanations)
- **Local**: Per-instance explanations with feature contribution weights
- Generates interpretable linear approximations around individual predictions

---

## ðŸ“œ License

This project is released under the MIT License for academic and research purposes.

---

## ðŸ“š References

1. Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. University of California, Irvine.
2. Lundberg, S.M. and Lee, S.I. (2017). A Unified Approach to Interpreting Model Predictions. *NeurIPS*.
3. Ribeiro, M.T., Singh, S. and Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. *KDD*.
